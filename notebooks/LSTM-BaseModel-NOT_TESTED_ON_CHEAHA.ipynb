{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Lambda, Embedding, GRU, Bidirectional, Concatenate, Dense\n",
    ")\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import gensim.downloader\n",
    "import re\n",
    "from calendar import day_name, day_abbr, month_name, month_abbr\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants that will be used as tokens\n",
    "PAD = '~PAD~'\n",
    "URL = '~URL~'\n",
    "DATETIME = '~DATETIME~'\n",
    "QUANTITY = '~QUANTITY~'\n",
    "NUM = '~NUM~'\n",
    "UNK = '~UNK~'\n",
    "\n",
    "# regexes to be applied prior to tokenization\n",
    "url_regex = '(www\\.|http://|https://)\\S+|\\S+\\.(co|org|net|info|be|gov|edu|html|jpg|jpeg|png|gif)'\n",
    "date_regex = '('        + '|'.join(day_name[i] for i in range(7)) +\\\n",
    "             '|'        + '|'.join(day_abbr[i] for i in range(7)) +\\\n",
    "             ')?,?\\s?(' + '|'.join(month_name[i] for i in range(1, 13)) +\\\n",
    "             '|'        + '|'.join(month_abbr[i] for i in range(1, 13)) +\\\n",
    "             \")\\s\\d{1,2}(st|nd|rd|th)?(,?\\s'?\\d{2,4})?\"\n",
    "time_regex = '((\\d+\\:\\d+)(\\s?[AaPp]\\.\\s?[Mm]\\.)?)|((\\d+\\:\\d+\\s?)?([AaPp]\\.\\s?[Mm]\\.))'\n",
    "quantity_regex = '\\d+((\\,?\\d+)+)?'\n",
    "\n",
    "# list of stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPERPARAMETERS ###\n",
    "PERCENT_TEST = 0.075\n",
    "PERCENT_VAL  = 0.075\n",
    "val_split = PERCENT_VAL / (1 - PERCENT_TEST)\n",
    "\n",
    "MAX_TEXT_LEN = 512\n",
    "MAX_TITLE_LEN = 64\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "### RANDOM SEED (for reproducibility) ###\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "seed = SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.5 s, sys: 1.54 s, total: 35.1 s\n",
      "Wall time: 37.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# download our pretrained word embedding with embedding size of 300\n",
    "EMBED_SIZE = 300\n",
    "gn300_raw = gensim.downloader.load('word2vec-google-news-300')\n",
    "gn300 = gn300_raw\n",
    "\n",
    "# \"\"\"\n",
    "# Condense our pretrained word embedding using the following two techniques:\n",
    "# 1) remove stopwords\n",
    "# 2) make all keys lowercase, group by key, and take average per key.\n",
    "# E.g. (gn300_raw['beyonce'] +\n",
    "#       gn300_raw['Beyonce'] +\n",
    "#       gn300_raw['BEYONCE']) / 3 -> gn300['beyonce']\n",
    "# \"\"\"\n",
    "# S = Counter() # vector sum\n",
    "# C = Counter() # vector count\n",
    "# for word in gn300_raw.vocab:\n",
    "#     lower = word.lower()\n",
    "#     if lower in stopwords: continue\n",
    "#     S[lower] += gn300_raw[word]\n",
    "#     C[lower] += 1\n",
    "# # create condensed word2vec (just a simple dictionary)\n",
    "# gn300 = {w: s / c for w, s, c in zip(S.keys(), S.values(), C.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "news = pd.read_csv('fake-news/train.csv')\n",
    "news = news.fillna(UNK)\n",
    "\n",
    "def get_real_fake(df):\n",
    "    real = df[df.label.isin([0, 'REAL'])]\n",
    "    fake = df[df.label.isin([1, 'FAKE'])]\n",
    "    return real, fake\n",
    "\n",
    "real, fake = get_real_fake(news)\n",
    "\n",
    "def split_df(df):\n",
    "    global seed\n",
    "    train, test = train_test_split(df, test_size=PERCENT_TEST, random_state=seed)\n",
    "    seed += 1\n",
    "    tr, val = train_test_split(train, test_size=val_split, random_state=seed)\n",
    "    seed += 1\n",
    "    return tr, val, test\n",
    "\n",
    "real_tr, real_val, real_test = split_df(real)\n",
    "fake_tr, fake_val, fake_test = split_df(fake)\n",
    "\n",
    "# def split_by_author(df):\n",
    "#     # take counts grouped by author\n",
    "#     author = df.author.value_counts()\n",
    "#     # split authors into groups\n",
    "#     author_tr, author_val, author_test = split_df(author)\n",
    "#     # split data by author group\n",
    "#     tr = df[df.author.isin(author_tr.index)].copy(deep=True)\n",
    "#     val = df[df.author.isin(author_val.index)].copy(deep=True)\n",
    "#     test = df[df.author.isin(author_test.index)].copy(deep=True)\n",
    "#     return tr, val, test\n",
    "\n",
    "# real_tr, real_val, real_test = split_by_author(real)\n",
    "# fake_tr, fake_val, fake_test = split_by_author(fake)\n",
    "\n",
    "# real_author_counts = real.author.value_counts()\n",
    "# fake_author_counts = fake.author.value_counts()\n",
    "# mixed_authors = real_author_counts.index.intersection(fake_author_counts.index)\n",
    "# real_author_counts[mixed_authors], fake_author_counts[mixed_authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8829 8851\n",
      "778 781\n",
      "780 781\n"
     ]
    }
   ],
   "source": [
    "def concat_and_shuffle(dfs):\n",
    "    global seed\n",
    "    df = pd.concat(dfs)\n",
    "    return df.sample(frac=1, random_state=seed)\n",
    "    seed += 1\n",
    "\n",
    "train = pd.concat([concat_and_shuffle([real_tr, fake_tr]),\n",
    "                   concat_and_shuffle([real_val, fake_val])])\n",
    "test = concat_and_shuffle([real_test, fake_test])\n",
    "\n",
    "# Expected result of val_split. This is just to give us a\n",
    "# sense of the distribution of labels.\n",
    "def display_count(df):\n",
    "    print(len(df[df.label.isin([0, 'REAL'])]),\n",
    "          len(df[df.label.isin([1, 'FAKE'])]))\n",
    "display_count(train.iloc[:-int(val_split * len(train))])\n",
    "display_count(train.iloc[-int(val_split * len(train)):])\n",
    "display_count(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def supertokenize(text):\n",
    "#     # e.g. split_by_case('ILoveNewYork') == ['I', 'Love', 'New', 'York']\n",
    "#     def split_by_case(s):\n",
    "#         if (not s[1:].isupper()) and (not s[1:].islower()):\n",
    "#             for i in range(1, len(s) - 1):\n",
    "#                 if s[i].isupper():\n",
    "#                     return [s[:i]] + split_by_case(s[i:])\n",
    "#                 elif s[i + 1].isupper():\n",
    "#                     return [s[:i + 1]] + split_by_case(s[i + 1:])\n",
    "#         return [s]\n",
    "#     text = re.sub(quantity_regex, QUANTITY, \n",
    "#                  re.sub(time_regex, DATETIME, \n",
    "#                         re.sub(date_regex, DATETIME, \n",
    "#                                re.sub(url_regex, URL, text))))\n",
    "#     text = text.replace('•', '')\n",
    "#     raw_tokens = nltk.word_tokenize(text)\n",
    "#     tokens = []\n",
    "#     for t in raw_tokens:\n",
    "#         if t in '!\"\\'(),--./:;<?[\\\\]{|}“”‘’—': continue\n",
    "#         if t.lower() != \"n't\": t = re.sub('[\\\"\\']', '', t)\n",
    "#         if (len(t) > 2) and (t.lower() == t) and (t[-1] == '.'): t = t[:-1]\n",
    "#         if t == URL:\n",
    "#             tokens.append(URL)\n",
    "#             continue\n",
    "#         if (t == DATETIME) or (re.sub('^\\d+(/\\d+)+$', '', t) == ''):\n",
    "#             tokens.append(DATETIME)\n",
    "#             continue\n",
    "#         if t == QUANTITY:\n",
    "#             tokens.append(QUANTITY)\n",
    "#             continue\n",
    "#         if t.isnumeric() or (t[:-2].isnumeric() and (t[-2:] in ['st', 'nd', 'rd', 'th'])):\n",
    "#             tokens.append(NUM)\n",
    "#             continue\n",
    "#         t = t.lower()\n",
    "#         if t in stopwords: continue\n",
    "#         if t in gn300:\n",
    "#             tokens.append(t)\n",
    "#         else:\n",
    "#             if t + '.' in gn300:\n",
    "#                 tokens.append(t + '.')\n",
    "#                 continue\n",
    "#             if t.replace('.', '') in gn300:\n",
    "#                 tokens.append(t.replace('.', ''))\n",
    "#                 continue\n",
    "#             is_found = False\n",
    "#             for x in split_by_case(re.sub('[^a-z]', '', t)):\n",
    "#                 if x in gn300:\n",
    "#                     tokens.append(x)\n",
    "#                     is_found = True\n",
    "#             if is_found: continue\n",
    "#             for st in re.split('-', re.sub('[^a-z0-9\\.~]', '-', t)):\n",
    "#                 for x in [st, st[:-1], st[1:], st.replace('s', 'z'), \n",
    "#                           st.replace('ou', 'o'), st.replace('re', 'er')]:\n",
    "#                     if x in gn300:\n",
    "#                         tokens.append(x)\n",
    "#                         is_found = True\n",
    "#                         break\n",
    "#             if not is_found: tokens.append(UNK)\n",
    "#     return tokens\n",
    "\n",
    "# this does tokenization as well as tracking token counts grouped by label\n",
    "def tokenize_and_update_word_counts(text, label, tokenize_fn):\n",
    "    global tc_real, tc_fake\n",
    "    tokens = tokenize_fn(text)\n",
    "    tc = Counter(tokens)\n",
    "    if label in [0, 'REAL']:\n",
    "        tc_fake += tc\n",
    "    elif label in [1, 'FAKE']:\n",
    "        tc_real += tc\n",
    "    else:\n",
    "        warnings.warn('Unexpected label %s'.format(label))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 3s, sys: 578 ms, total: 5min 3s\n",
      "Wall time: 5min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25547,\n",
       " [('Mr.', 235.36549487441647),\n",
       "  (',', 234.03912592216693),\n",
       "  ('said', 191.65634963518934),\n",
       "  ('.', 185.17031714126486),\n",
       "  ('”', 177.47049185041664),\n",
       "  ('“', 173.86553560719304),\n",
       "  ('a', 162.61947762656303),\n",
       "  ('the', 146.5089498487329),\n",
       "  ('’', 142.23684361361728),\n",
       "  ('—', 137.04071200784927)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tc_real=Counter()\n",
    "tc_fake=Counter()\n",
    "\n",
    "TOKENIZE_FN = nltk.word_tokenize\n",
    "\n",
    "train['tokenized_title'] = train.apply(lambda x, y: tokenize_and_update_word_counts(\n",
    "    x.title, x.label, y), axis=1, args=(TOKENIZE_FN,))\n",
    "train['tokenized_text'] = train.apply(lambda x, y: tokenize_and_update_word_counts(\n",
    "    x.text, x.label, y), axis=1, args=(TOKENIZE_FN,))\n",
    "\n",
    "T = Counter()\n",
    "for k in tc_real + tc_fake:\n",
    "    T[k] = abs(tc_real[k] - tc_fake[k]) / np.sqrt(tc_real[k] + tc_fake[k])\n",
    "\n",
    "thresh = T.most_common(25000)[-1][1]\n",
    "tokens = [k for k, v in T.items() if v >= thresh]\n",
    "\n",
    "vocab, i = {PAD: 0, URL: 1, DATETIME: 2, QUANTITY: 3, NUM: 4, UNK: 5}, 6\n",
    "for w in tokens:\n",
    "    if w in (PAD, URL, DATETIME, QUANTITY, NUM, UNK): continue\n",
    "    vocab[w] = i\n",
    "    i += 1\n",
    "id2word = {v: k for k, v in vocab.items()}\n",
    "\n",
    "i, T.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.65 s, sys: 20 ms, total: 7.67 s\n",
      "Wall time: 7.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test['tokenized_title'] = test.apply(lambda x: TOKENIZE_FN(x.title) ,axis=1)\n",
    "test['tokenized_text'] = test.apply(lambda x: TOKENIZE_FN(x.text), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text,length):\n",
    "    f = [vocab[w] if w in vocab else vocab[UNK] for w in text]\n",
    "    f = f[:length]\n",
    "    return (length - len(f)) * [vocab[PAD]] + f\n",
    "train['text_as_ints'] = train.tokenized_text.apply(clean, args=[MAX_TEXT_LEN])\n",
    "train['title_as_ints'] = train.tokenized_text.apply(clean, args=[MAX_TITLE_LEN])\n",
    "test['text_as_ints'] = test.tokenized_text.apply(clean, args=[MAX_TEXT_LEN])\n",
    "test['title_as_ints'] = test.tokenized_text.apply(clean, args=[MAX_TITLE_LEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [np.array(train.text_as_ints.to_list()), np.array(train.title_as_ints.to_list())]\n",
    "X_test = [np.array(test.text_as_ints.to_list()), np.array(test.title_as_ints.to_list())]\n",
    "\n",
    "# binarize labels\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "y_train = lb.fit_transform(train.label)\n",
    "y_test = lb.fit_transform(test.label)\n",
    "\n",
    "# compute class weights based on training labels\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=[0, 1],\n",
    "                                                  y=[w for x in y_train for w in x])\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    7664100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None)         0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    7664100     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 32)           30528       embedding[0][0]                  \n",
      "                                                                 lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 16)           15264       embedding_1[0][0]                \n",
      "                                                                 lambda[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 48)           0           bidirectional[0][0]              \n",
      "                                                                 gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            49          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 15,374,041\n",
      "Trainable params: 15,374,041\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_text = Input(shape=[None])\n",
    "input_title = Input(shape=[None])\n",
    "mask = Lambda(lambda inputs: K.not_equal(inputs, 0))\n",
    "def generate_embedding():\n",
    "    emb = np.zeros((len(vocab), EMBED_SIZE))\n",
    "#     v = np.random.uniform(-1, 1, EMBED_SIZE)\n",
    "    for i in range(len(vocab)):\n",
    "        try:\n",
    "            g = gn300[id2word[i]]\n",
    "            emb[i] = g #+ v * np.linalg.norm(g) / np.linalg.norm(v)\n",
    "        except KeyError:\n",
    "            pass\n",
    "#             assert i <= 5, i\n",
    "#             if i in [1, 2, 3, 4]: # leave PAD and UNK zero\n",
    "#                 e[i] = v / np.linalg.norm(v)\n",
    "    return Embedding(input_dim=emb.shape[0],\n",
    "                     output_dim=emb.shape[1],\n",
    "                     weights=[emb],\n",
    "#                      trainable=False\n",
    "                    )\n",
    "emb_text = generate_embedding()(input_text)\n",
    "emb_title = generate_embedding()(input_title)\n",
    "gru = lambda: GRU(16, dropout=0.2, recurrent_dropout=0.2,\n",
    "                  kernel_regularizer=L1L2(l1=1e-6, l2=2e-6))\n",
    "gru_text = Bidirectional(gru())(emb_text, mask=mask(input_text))\n",
    "gru_title = gru()(emb_title, mask=mask(input_title))\n",
    "concat = Concatenate()([gru_text, gru_title])\n",
    "outputs = Dense(1, activation='sigmoid')(concat)\n",
    "model = tf.keras.Model(inputs=[input_text, input_title], outputs=[outputs])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics='accuracy')\n",
    "# model.fit(X_train, y_train, batch_size=BATCH_SIZE, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(model.layers)):\n",
    "#     model.layers[i].trainable = False\n",
    "#     if i in [2, 4]:\n",
    "#         model.layers[i].trainable = True\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics='accuracy')\n",
    "# model.fit(X_train, y_train, batch_size=BATCH_SIZE, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(model.layers)):\n",
    "#     model.layers[i].trainable = True\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "277/277 [==============================] - 221s 779ms/step - loss: 0.6809 - accuracy: 0.5686 - val_loss: 0.6551 - val_accuracy: 0.6859\n",
      "Epoch 2/5\n",
      "277/277 [==============================] - 219s 789ms/step - loss: 0.6439 - accuracy: 0.7121 - val_loss: 0.6210 - val_accuracy: 0.7718\n",
      "Epoch 3/5\n",
      "277/277 [==============================] - 219s 792ms/step - loss: 0.6075 - accuracy: 0.7815 - val_loss: 0.5858 - val_accuracy: 0.8160\n",
      "Epoch 4/5\n",
      "277/277 [==============================] - 212s 765ms/step - loss: 0.5730 - accuracy: 0.8127 - val_loss: 0.5487 - val_accuracy: 0.8442\n",
      "Epoch 5/5\n",
      "277/277 [==============================] - 211s 761ms/step - loss: 0.5351 - accuracy: 0.8379 - val_loss: 0.5094 - val_accuracy: 0.8558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efefc4b7580>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics='accuracy')\n",
    "checkpoint_filepath = '/tmp/checkpoint_v0'\n",
    "model.fit(\n",
    "    X_train, y_train, batch_size=BATCH_SIZE, class_weight=class_weights,\n",
    "    validation_split=val_split, epochs=EPOCHS,\n",
    "    callbacks=[tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [0.79709387]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('For Helping Immigrants, Chobani’s Founder Draws Threats - The New York Times',\n",
       " 'By many measures, Chobani embodies the classic American immigrant success story. Its founder, Hamdi Ulukaya, is a Turkish immigrant of Kurdish descent. He bought a defunct yogurt factory in upstate New York, added a facility in Twin Falls, Idaho, and now employs about 2, 000 people making Greek yogurt. But in this contentious election season, the extreme right has a problem with Chobani: In its view, too many of those employees are refugees. As Mr. Ulukaya has stepped up his advocacy  —   employing more than 300 refugees in his factories, starting a foundation to help migrants, and traveling to the Greek island of Lesbos to witness the crisis firsthand  —   he and his company have been targeted with racist attacks on social media and conspiratorial articles on websites including Breitbart News. Now there are calls to boycott Chobani. Mr. Ulukaya and the company have been taunted with racist epithets on Twitter and Facebook. Fringe websites have published false stories claiming Mr. Ulukaya wants “to drown the United States in Muslims. ” And the mayor of Twin Falls has received death threats, partly as a result of his support for Chobani. Online hate speech is on the rise, reflecting the rising nationalism displayed by some supporters of Donald J. Trump, who has opposed resettling refugees in the United States. “What’s happening with Chobani is one more flash point in this battle between the voices of xenophobia and the voices advocating a rational immigration policy,” said Cecillia Wang, director of the Immigrants’ Rights Project at the American Civil Liberties Union. Chobani and Mr. Ulukaya declined to comment for this article. The Trump campaign did not reply to a request for comment. Mr. Ulukaya arrived in upstate New York in the 1990s to attend school. By 2002, he was making and selling feta cheese inspired by a family recipe. A few years later, he learned that a local yogurt and cheese factory that had closed was for sale. He received a loan of $800, 000 from the Small Business Administration to purchase the factory, and started selling Chobani yogurt in 2007. As the business grew, Mr. Ulukaya needed more help. When he learned there was a refugee resettlement center in a nearby town, he asked if any of the newcomers wanted jobs at Chobani. Mr. Ulukaya provided transportation for the new hires, and he brought in translators to assist them. He paid the refugee workers salaries above the minimum wage, as he did other workers at the factory. When Chobani opened its factory in Twin Falls, Mr. Ulukaya once again turned to a local resettlement center. The company now employs resettled refugees from Iraq, Afghanistan and Turkey, among other countries. “The minute a refugee has a job, that’s the minute they stop being a refugee,” Mr. Ulukaya said in a talk he gave this year. Today, Chobani has annual yogurt sales of around $1. 5 billion. Last year, Mr. Ulukaya signed the Giving Pledge, promising to give away a majority of his fortune to assist refugees. Chobani and the other companies working with refugees are not exploiting them, said Jennifer Patterson, project director for the Partnership for Refugees, a federal program. “It’s the exact opposite,” Ms. Patterson said. “These companies are looking to provide resettled refuges with the ability to live happy and productive lives. ” Chobani’s work with refugees went largely unnoticed until this January, when Mr. Ulukaya spoke at the World Economic Forum in Davos, Switzerland. His message  —   that corporations needed to do more to assist refugees  —   broke through the   rhetoric. “He was quite a sensation there,” said Kenneth Roth, executive director of Human Rights Watch, who attended the event. “Here was someone who went beyond the   chatter of Davos and was walking the walk. ” Cisco, IBM, Salesforce and more joined others in pledging assistance to refugees. Those companies and others began working with the Tent Foundation, which Mr. Ulukaya founded last year. Chobani has pledged to help other companies learn how to effectively integrate refugees into a work force. But while an alliance of   companies was now working together on the issue, the online critics zeroed in on Chobani. Shortly after Mr. Ulukaya spoke in Davos, the   website WND published a story originally titled “American Yogurt Tycoon Vows to Choke U. S. With Muslims. ” Then this summer, Breitbart, the conservative news website whose former executive chairman, Stephen K. Bannon, is now running the Trump campaign, began publishing a series of misleading articles about Chobani. One drew a connection between Chobani’s hiring of refugees and a spike in tuberculosis cases in Idaho. Another linked Chobani to a “Twin Falls Crisis Imposed by     Advocates. ” A third conflated Chobani’s hiring practices with a sexual assault case in Twin Falls involving minors. As Breitbart began publishing its articles, the online attacks grew more intense. On Twitter and Facebook, users called for a boycott of Chobani. An image was widely shared on social media that claimed Mr. Ulukaya was “going to drown the United States in Muslims and is importing them to Idaho 300 at a time to work in his factory. ” And bloggers fabricated stories claiming that Chobani was pressuring local officials “to facilitate their multitude of Muslim   requests. ” Soon the mayor of Twin Falls, Shawn Barigar, found himself at the center of a conspiracy theory. “It got woven into a narrative that it’s all a   that we’re all trying to keep the refugees safe so that Chobani has its work force, that I personally am getting money from the Obama administration to help Chobani hire whoever they want, that it’s part of this Islamification of the United States,” he said. “It’s crazy. ” As the online comments escalated this summer, Mr. Barigar and his wife received death threats. Breitbart said it was simply covering the news. “Breitbart has been a leader in delivering important and breaking news on refugee crises throughout the Western world, which pose both national security and financial risks,” Alex Marlow, editor in chief, said in a statement. “Mr. Ulukaya hasn’t merely involved himself in this issue, he’s been one of the leaders in expanding refugee resettlement in the United States. Breitbart’s explosive growth is due in large measure to the mainstream media’s refusal to cover vital topics like this one. ” But civil rights advocates said they believed it was no mystery why Mr. Ulukaya was targeted while other chief executives had been spared. “It’s because he’s an immigrant himself,” Ms. Wang of the A. C. L. U. said. Mr. Roth of Human Rights Watch attributed some of the xenophobia directed at Chobani to the election season. “Some people are feeling left behind, and some people are concerned about terrorists,” he said. “But Trump has given a voice to these sentiments. ” Mr. Barigar, a Democrat, concurred. “Donald Trump really fueled a sentiment about immigration that is shared by a very small part of our community,” he said. “We are an agricultural center. We’ve depended on immigrants for a   or more. ” Mr. Ulukaya appears undeterred. In September, he participated in a   discussion with President Obama and business leaders on how corporations could do more to help refugees. And his work with refugees is part of a broader suite of initiatives. He recently gave 10 percent of Chobani shares to his employees, and he is offering paid parental leave to all employees. “He’s the xenophobe’s nightmare,” Mr. Roth said. “Here’s an immigrant who isn’t competing for jobs, but is creating jobs big time. It runs completely counter to the   narrative. ”')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(checkpoint_filepath)\n",
    "logits = model.predict(X_test)\n",
    "most_off = np.argmax(np.abs(y_test - logits))\n",
    "print(y_test[most_off], logits[most_off])\n",
    "test.iloc[most_off].title, test.iloc[most_off].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7160742996593131\n",
      "[[704  76]\n",
      " [148 633]]\n",
      "49/49 [==============================] - 4s 75ms/step - loss: 0.5084 - accuracy: 0.8565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5083625316619873, 0.8565022349357605]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.round(logits)\n",
    "print(matthews_corrcoef(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reshaped_weights(model,layer_id):\n",
    "    W=np.array(model.layers[layer_id].get_weights())\n",
    "    W=W.reshape(W.shape[1],W.shape[2])\n",
    "    return W\n",
    "text_embed = get_reshaped_weights(model,2)\n",
    "title_embed = get_reshaped_weights(model,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-ecfad56ed059>:8: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  cs = W.dot(b) / (np.linalg.norm(W, axis=1) * np.linalg.norm(b))\n",
      "<ipython-input-21-ecfad56ed059>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cs = W.dot(b) / (np.linalg.norm(W, axis=1) * np.linalg.norm(b))\n",
      "<ipython-input-21-ecfad56ed059>:4: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  n_ = np.sum(arr >= thresh)\n",
      "/home/christophercoffee/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 146 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/christophercoffee/anaconda3/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 146 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_top_n_plus_ties(arr, n):\n",
    "    sorted_args = np.argsort(-arr)\n",
    "    thresh = arr[sorted_args[n]]\n",
    "    n_ = np.sum(arr >= thresh)\n",
    "    return sorted_args[:n_]\n",
    "def get_most_similar(word, W, n):\n",
    "    b = W[vocab[word]]\n",
    "    cs = W.dot(b) / (np.linalg.norm(W, axis=1) * np.linalg.norm(b))\n",
    "    return get_top_n_plus_ties(cs, n)\n",
    "def create_words_plot(embed1, embed2, WORD, k=9):\n",
    "    global seed\n",
    "    ids1 = get_most_similar(WORD, embed1, k-1)\n",
    "    ids2 = get_most_similar(WORD, embed2, k-1)\n",
    "    words1 = [id2word[i] for i in ids1]\n",
    "    words2 = [id2word[i] for i in ids2]\n",
    "    v = TSNE(n_components=2,random_state=seed).fit_transform(\n",
    "        np.concatenate([embed1[ids1],embed2[ids2]]))\n",
    "    seed += 1\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    plt.scatter(v[:k,0],v[:k,1],facecolors='none',edgecolors='teal')\n",
    "    plt.scatter(v[k:,0],v[k:,1],facecolors='none',edgecolors='red')\n",
    "    for i, word in enumerate(words1):\n",
    "        plt.annotate(word,(v[i,0],v[i,1]),size=9.5)\n",
    "        if word == WORD:\n",
    "            plt.scatter(v[i,0],v[i,1],c='teal',s=300)\n",
    "    for i, word in enumerate(words2):\n",
    "        plt.annotate(word,(v[i+k,0],v[i+k,1]),size=9.5)\n",
    "        if word == WORD:\n",
    "            plt.scatter(v[i+k,0],v[i+k,1],c='red',s=300)\n",
    "create_words_plot(text_embed,title_embed,'headline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
